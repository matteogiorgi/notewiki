```
Introduction to Machine Learning

Learning: universal principles for living beings, society, machines

What is ML? 
‚Ä¢ Learning: a complex aim, a continuously growing research field
‚Ä¢ In Computer Science, theoretical and applicative field called:
	‚Äì Apprendimento Automatico in italiano (it)
	‚Äì Machine Learning (ML) English and literature (aka learning systems)
‚Ä¢ Machine Learning has emerged as an area of research combining the aims of creating computers 
that could learn (IA) and new powerful adaptive/statistical tools with rigorous foundation 
in computational science
‚Ä¢ Machines that learn by themselves. Why? Luxury or necessity?
	‚Äì Growing availability and need for analysis of empirical data
		‚Üí Central/methodological role due to changing of paradigm in science: data-driven
	‚Äì Difficult to provide adaptivity/intelligence by programming [see Turing]
		‚Üí learning as the only choice...
‚Ä¢ DATA + HPC + modern ML ‚Üí Pave the way to a new AI era

Aims include:
‚Ä¢ As AI methodology ‚Üí Build Adaptive Intelligent Systems
	‚Äì from search engine to robotics...
‚Ä¢ As statistical learning
	‚Üí Build powerful predictive system for Intelligent Data Analysis
	‚Äì tools for the ‚Äúdata scientist‚Äù
‚Ä¢ As computer science method for innovative application areas
	‚Üí Using models as a tool for complex (interdisciplinary) problems
	‚Äì from biological data analysis to image understanding...

Simple concrete examples
‚Ä¢ Automatized learning by the system of the experience (set of examples) to address a computational task
	- Email spam classificaiton, character/face/speech recognition
‚Ä¢ no (or poor) prior knowledge/rules for the solution but it is (more) easy 
to have a source of training experience (data with known results)
‚Ä¢ Models used in Real-World systems (pervasive*) and in new interdisciplinary area, encompassing:
	‚Äì Pattern Recognition, Robotics, Computer Vision, Natural Language Processing,
	Information Retrieval, Web search engine, Complex Analyses of Data (Med, Bio, Web), 
	Data Mining, Financial forecasting, Adaptive Systems and Filters, Intelligent Sensor Networks 
	(Smart IoT), Personalized components...

An instance on a ‚Äúrecent‚Äù result (2014)
‚Ä¢ Face recognition combining (deep) Neural Networks and other ML approaches
‚Ä¢ Starting form four million facial images belonging to more than 4,000 identities
‚Ä¢ Asked whether two photos show the same person, DeepFace answers correctly 97.25% of the time...
just a shade behind humans (97.53%).

Machine Learning: when? (summarizing)
Opportunity (if useful) and awareness (needs and limits)
‚Ä¢ Utility of predictive learning models: (in the following cases)
	‚Äì no (or poor) theory (or knowledge to explain the phenomenon or difficult to be formalized)
	‚Äì uncertain, noisy or incomplete data (which hinder formalization of solutions)
	‚Äì dynamical environments, not known in advance (E.g. adapt to personalized behavior 
	according to dynamical user preferences)
‚Ä¢ Requests:
	- source of training experience (representative data)
	- tolerance on the precision of results *

Machine Learning: why?
‚Ä¢ An opportunity to know new computing paradigms, with an approach different 
w.r.t. to Standard Programming, algorithmic/classic IA approaches
	‚Äì e.g. treatment of uncertainty, tolerance of imprecision...
‚Ä¢ Typical of the soft computing/computational intelligence area
‚Ä¢ To find approximate solutions for difficult problems, difficult to be formalized by ‚Äòhand-made‚Äô algorithm
‚Ä¢ To build new robust and wide applicable intelligent systems
‚Ä¢ But it is NOT an approximate methodology!
‚Ä¢ It is a rigorous approach to find approximate function to deal with complex problems 
(supported by empirical and theoretical results e.g. SLT),
	‚Äì soft computing paradigm: open many new opportunities (extend the frontier of CS applications)
	‚Äì Often bio-inspired (neural) modeling

Overview of a ML System:
	[Img. 10-1]

Learning as an approximation of an unknown function from examples
Specific vision but widespread in ML. For us:
	‚ñ™ Different tasks seen in uniform framework
	‚ñ™ Enables a rigorous formulation
		‚Üí Intro guided by an intuitive example

An Example
‚Ä¢ A pilot example: recognition of handwritten digits
‚Ä¢ Input: collection of images of handwritten digits (arrays/matrix of values)
‚Ä¢ Problem: build model that receives in input an image of handwritten digit and "predict" the digits
	- Classification problem
‚Ä¢ Difficult to formalize exactly the solution of the problem: possible presence of noise and ambiguous data;
‚Ä¢ Relatively easy to collect collection of labeled examples
	=> Example of successful application of the ML!

A step ahead from the pilot example
Generalizing the pilot example problem:
‚Ä¢ Supervised learning (classification, regression)
	x (Input space vectors) -> (f, function built from examples) -> Categories or real values (R)

Tasks: Supervised Learning
‚Ä¢ Given: Training examples as <input, output> = <x, d> (labeled examples)
for an unknown function f (known only at the given points of example)
	‚Äì Target value: desiderate value d or t or y... is given by the teacher according to f(x).
‚Ä¢ Find: A good approximation to f (a hypothesis h that can used for prediction on unseen data x')
‚Ä¢ Target d (or t or y): a categorical or numerical label
	‚Äì CLASSIFICATION: f(x) return the (assumed) correct class for x
		f(x) is a discrete-valued function ‚àà {1, 2, ..., k} classes
	‚Äì REGRESSION: approximate a real-valued target function (in R or R^K)
Both as a function approximation task

Examples of x - f(x)
Inferring general functions from know data:
‚Ä¢ Handwriting Recognition
	‚Äì x: Data from images of the characters.
	‚Äì f(x): Letter of the alphabet.
‚Ä¢ Disease diagnosis (from database of past medical records)
	‚Äì x : Properties of patient (symptoms, lab tests)
	‚Äì f(x): Disease (or maybe, recommended therapy)
	‚Äì TR (training set) <x, f(x)>: database of past medical records
‚Ä¢ Face recognition
	‚Äì x: Bitmap picture of person's face
	‚Äì f(x): Name of the person.
‚Ä¢ Spam Detection
	‚Äì x: Email message
	‚Äì f(x): Spam or not spam.

Tasks: Unsupervised Learning
Unsupervised Learning: No teacher!
‚Ä¢ TR (Training Set) = set of unlabeled data <x>
‚Ä¢ E.g. to find natural groupings in a set of data
	‚Äì Clustering
	‚Äì Dimensionality reduction/Visualization/Preprocessing
	‚Äì Modeling the data density

‚Ä¢ Clustering: Partition of data into clusters (subsets of ‚Äúsimilar‚Äù data)
	[Img. 10-2]

Models and survey of useful concepts
‚Ä¢ MODEL:
	‚Äì Aim: to capture/describes the relationships among the data (on the basis of the task)
	‚Äì It defines the class of functions that the learning machine can implement (hypothesis space)
		‚Ä¢ E.g. set of functions h(x, w), where w is the (abstract) parameter
‚Ä¢ Training example (superv.): An example of the form (x, f(x) + noise)
	x is usually a vector of features, (d or t or) y = f(x) + noise is called the target value
‚Ä¢ Target function: The true function f
‚Ä¢ Hypothesis: A proposed function h believed to be similar to f.
An expression in a given language that describes the relationships among data
‚Ä¢ Hypotheses space: The space of all hypotheses (specific models) 
that can, in principle be output by the learning algorithm

Models Languages for H
‚Ä¢ alcuni linguaggi in cui esprimere relazioni che possiamo usare per esprimere modelli di ML (le ipotesi h):
	- Logica del primo ordine
	- Equazioni numeriche
	- Probabilit√†: questo verr√† reintrodotto per la rappresentazione della conoscenza incerta in AI 
	(e quindi per esprimere modelli di ML)

Models: few simple examples...
Just to have a preview of different representation of hypothesis:

‚Ä¢ Linear models (representation of H defines a continuously parameterized space of potential hypothesis);
each assignment of w is a different hypothesis, e.g:
	- hw(x) = w1x + w0 
	E.g. hw(x) = 0.232x + 246

‚Ä¢ Symbolic Rules: (hypothesis space is based on discrete representations);
different rules are possible, e.g: 
	‚Äì if (x1 == 0) and (x2 == 1) then h(x) = 1
	‚Äì else h(x)= 0

‚Ä¢ Probabilistic models: estimate p(x, y)
‚Ä¢ Instance based approaches: Predict mean y value of nearest neighbors (memory-based)

Learning Algorithms
‚Ä¢ LEARNING ALGORITHM
‚Ä¢ Basing on data, task and model, learning as a (Heuristic) search through the hypothesis space H 
of the best hypothesis = the best approximation to the (unknown) target function.
	‚Äì i.e. the best approximation to the (unknown) target function
‚Ä¢ Typically searching for the h with the minimum ‚Äúerror‚Äù
‚Ä¢ E.g. free parameters of the model are fitted to the task at hand:
	‚Äì Examples: best w in linear models, best rules for symbolic models, ...
‚Ä¢ H may not coincide with the set of all possible functions and the search can not be exhaustive: 
it needs to make assumptions ‚Üí we will see the role of the Inductive bias

Machine Learning: generalization
‚Ä¢ Learning: search for a good function in a function space from known data 
(typically minimizing an Error/Loss)
‚Ä¢ Good w.r.t. generalization error: it measures how accurately the model predicts 
over novel samples of data (Error/Loss measured over new data) (low error, high accuracy and vice versa)

Generalization: crucial point of ML!!! Easy to use ML tools versus correct/good use of ML

Generalization
‚Ä¢ Learning phase (training, fitting): build the model from know data ‚Äì training data (and bias)

‚Ä¢ Predictive phase (test): apply to new example (we take the input x; we compute the response by the model;
we compare with its target that the model has never seen): evaluation of the predictive hypothesis, 
i.e. of the generalization capability

Note: performance in ML = predictive accuracy 
	-> estimated by the error computed on the (Hold out) Test Set
‚Ä¢ Theory: E.g. Statistical Learning Theory [Vapnik]:
	‚Äì under what (mathematical) conditions is a model able to generalize? 

Concept Learning

Overview
‚Ä¢ Concept Learning from examples as research in the Hypothesis Space
	‚Äì an opportunity to have a look of a simple symbolic learning system (propositional) (rule based)
	‚Äì as the basis for the subsequent introduction of flexible models
‚Ä¢ Discrete space (symbolic representation of the target function), structure of the hypothesis space
‚Ä¢ Representation: examples: Conjunction of literals
‚Ä¢ Search Alg.: Find-S, candidate elimination
‚Ä¢ Inductive Bias

Learning Problem
‚Ä¢ Learning as Improving with experience at some task
‚Ä¢ Improve over task T, with respect to performance measure P, based on experience E.

Supervised learning
‚Ä¢ Given: Training examples as <input, output> = <x, d>
for an unknown function f (known only at the given points of example)
	‚Äì Target value: desiderate value d or t or y... is given by the teacher according to f(x).
‚Ä¢ Find: A good approximation to f (a hypothesis h that can used for prediction on unseen data x‚Äô)
‚Ä¢ Target d (or y or t): a categorical or numerical label
	‚Äì Classification: f(x) returns the (assumed) correct class for x 
				-> f(x) is a discrete-valued function
	‚Äì Regression: approximate a real-valued target function (in R o R^K)
Both as a task in function approximation

Now we see: CLASSIFICATION

Concept Learning
‚Ä¢ Concept Learning: inferring a Boolean function from positive and negative training examples
‚Ä¢ C: X ‚Üí {t,f} or {+,-} or {yes, no} or {0,1}, X: instance space
‚Ä¢ Examples of concepts: category ‚Äúcat‚Äù or ‚Äúenjoy-sport‚Äù

‚Ä¢ Def: ‚ÄúExample‚Äù: <x, c(x)> in D (o TR set)
‚Ä¢ Def: h: X ‚Üí {0, 1} satisfies x if h(x) = 1
‚Ä¢ Def: a hypothesis h is consistent with
	‚Äì An example <x, c(x)>, x in X, if h(x) =c (x)
	‚Äì D, if h(x) = c(x) for each training example <x, c(x)> in D.

An example:
Learning Boolean functions

This is an ill posed (inverse) problem:
We may violate either existence, uniqueness, stability of the solution or solutions

‚Ä¢ There are 2^16 = 2^(2^4) = 65536 possible Boolean functions over four input features. 
We can't figure out which one is correct until we've seen every possible input-output pair.
‚Ä¢ After 7 examples, we still have 2^9 (512) possibilities.
‚Ä¢ In the general case: |H| = 2^(#instances) = 2^(2^n) for binary inputs/outputs, n = input dimension

Guide to next solutions
‚Ä¢ Work with a restricted hypothesis space: we start choosing a space of hypotheses H 
that is considerably smaller than the space of all possible functions! (language bias)
‚Ä¢ We will see:
	- (Simple) conjunctive rules (in a finite discrete H)
	- Linear functions (in an infinite continuous H)

‚Ä¢ How to organize and efficiently search through a (discrete) space of hypothesis: 
some algorithms for a very restricted set of hypotheses
	‚Äì Conjunctive rules
	‚Äì No noisy data

Conjunctive Rules
‚Ä¢ How many distinct h? = How many simple conjunctive rules?
	‚Äì 4 inputs: True, 4 single, 6 couple, 4 triple, 1 quadruple: 16
‚Ä¢ In the general case:
	- Positive literals, e.g.: h1 = l2, h2 = (l1 and l2), h3 = true...
		‚Äì> Simple conjunctive rules
		‚Äì> |H| = 2^n (image li as a bit of a string of length n)
	- Literals (also not(li))
		‚Äì> |H| = 3^n + 1 (why?)

Representing Hypothesis
‚Ä¢ Hypothesis h is a conjunction of constraints on attributes
‚Ä¢ Each constraint can be:
	‚Äì A specific value: e.g. Water=Warm
		‚Äì A don‚Äôt care value : e.g. Water=?
	‚Äì No value allowed (null hypothesis): e.g. Water=√ò (e.g. li AND not(li))
‚Ä¢ Example: hypothesis h (h in the following)
	Sky Temp Humid Wind Water Forecast
	< Sunny ? ? Strong ? Same>
Corresponding to boolean function:
	Sky=Sunny ‚àß (and) Wind=Strong ‚àß (and) Forecast=Same

< √ò √ò √ò √ò √ò √ò > Most specific
< ? ? ? ? ? ? > Most general

Prototypical Concept Learning Task
Given:
‚Ä¢ Instances X: Possible days decribed by the attributes Sky, Temp, Humidity, Wind, Water, Forecast
‚Ä¢ Target function c: EnjoySport X ‚Üí {0,1}
‚Ä¢ Hypotheses H: conjunction of (a finite set of) literals e.g.
	< Sunny ? ? Strong ? Same >
‚Ä¢ Training examples D : positive and negative examples of the target function: 
	<x1, c(x1)>, ..., <xn, c(xl)>
Find:
‚Ä¢ A hypothesis h in H such that h(x)=c(x) for all x in X.

Learning: searching in the hypotheses space H

Inductive Learning Hypothesis
‚Ä¢ Assumption here: ‚ÄùAny hypothesis found to approximate the target function well over the training examples,
will also approximate the target function well over the unobserved examples‚Äù.
‚Ä¢ h(x)=c(x) for all x in D (i.e. consistent with D)
‚Ä¢ h(x)=c(x) for all x in X (?)
‚Ä¢ (?) = I.e. The Foundamental issue of the ML:
	‚Äì theoretical and empirical analysis

Number of Instances, Concepts, Hypotheses
Esempio:
‚Äì Sky: Sunny, Cloudy, Rainy (3 values!)
‚Äì AirTemp: Warm, Cold
‚Äì Humidity: Normal, High
‚Äì Wind: Strong, Weak
‚Äì Water: Warm, Cold
‚Äì Forecast: Same, Change
‚Ä¢ The representation chosen for H determines the search space

#distinct instances: 3*2*2*2*2*2 = 96
#distinct concepts: 2^96 = 2^(#instances)
#syntactically distinct hypotheses (conjunctions): 5*4*4*4*4*4=5120
#semantically distinct hypotheses: 1+4*3*3*3*3*3=973
(because all h with √ò are equivalen to "false")

In general: high dimension, even infinite: 
structuring the search space may help in searching more efficiently

General to Specific Ordering
‚Ä¢ Consider two hypotheses:
	‚Äì h1 = <Sunny,?,?,Strong,?,?>
	‚Äì h2 = <Sunny,?,?,?,?,?>
‚Ä¢ Set of instances covered by h1 and h2:
	h2 imposes fewer constraints than h1 and therefore classifies more instances x as positive 
	[i.e. h(x)=1].

Def: Let hj and hk be boolean-valued functions defined over X. 
Then hj is more general than or equal to hk (written hj >= hk) if and only if
	Per ogni x appartenente a X : [(hk(x) = 1) ‚Üí (hj(x) = 1)]
Examples over binary:
	- li: l1 >= (l1 and l2)
	- l1 versus l2: not comparable
‚Ä¢ The relation >= imposes a partial order (p.o.) over the hypothesis space H 
that is utilized by many concept learning methods.
‚Ä¢ We can take advantage of this p.o. to efficently organize the search in H

Find-S Algorithm
Exploit m.g.t. partial order to efficiently search consistent h (without explicitly enumerating every h in H)

1. Initialize h to the most specific hypothesis in H
2. For each positive training instance x
	For each attribute ai in h
	If the ai in h is satisfied by x 
		then do nothing
	else replace ai in h by the next more general constraint that is satisfied by x 
	[e.g. remove from h literals not satisfying x]
3. Output hypothesis h

Properties of Find-S
‚Ä¢ Hypothesis space described by conjunctions of attributes (hard limitations!)
‚Ä¢ Find-S will output the most specific hypothesis within H
that is consistent with the positive training examples
‚Ä¢ The output hypothesis h will also be consistent with the negative examples, 
provided the target concept is contained in H.
‚Äì Because c >= h

Complaints about Find-S
‚Ä¢ Can‚Äôt tell if the learner has converged to the target concept, in the sense that it is unable 
to determine whether it has found the only hypothesis consistent with the training examples.
‚Ä¢ Can‚Äôt tell when training data is inconsistent, as it ignores negative training examples: 
no noise tollerance!
‚Ä¢ Why prefer the most specific hypothesis?
‚Ä¢ What if there are multiple maximally specific hypothesis?

Version Spaces
Key idea: ouput a description of the set of all h consistent with D
We can do it without enumerating them all
‚Ä¢ Consistent(h,D) := Per ogni <x,c(x)> appartenente a D, h(x)=c(x)
‚Ä¢ The version space, VS_H,D, with respect to hypothesis space H and training set D, 
is the subset of hypotheses from H consistent with all training examples:
VSH,D = {h appartenente a H | Consistent(h,D)}

List-Then Eliminate Algorithm
1. VersionSpace a list containing every hypothesis in H
2. For each training example <x,c(x)>
remove from VersionSpace any hypothesis that is inconsistent
with the training example h(x) != c(x)
3. Output the list of hypotheses in VersionSpace
Irrealistic: exuastive enumeration of all h in H

Representing Version Spaces [Img. 11-1]
‚Ä¢ The general boundary, G, of version space VS_H,D is the set of maximally general members 
(of H consistent with D).
‚Ä¢ The specific boundary, S, of version space VS_H,D is the set of maximally specific members 
(of H consistent with D).
‚Ä¢ Theorem: Every member of the version space lies between these boundaries
	VS_H,D = {h appartenente a H| (Esiste s appartenente a S) (Esiste g appartenente a G) 
		(g >= h >= s)
	where x >= y means x is more general or equal than y

We can make a complete search of consistent hypotheses using the two boundaries S and G for the VS,
i.e. not restricted to S as for Find-S.

Candidate Elimination Algorithm
G <- maximally general hypotheses in H
S <- maximally specific hypotheses in H

For each training example d=<x,c(x)>:
If d is a positive example:
	Remove from G any hypothesis that is inconsistent with d (def. of VS)
	For each hypothesis s in S that is not consistent with d [generalize S]:
		‚Ä¢ remove s from S.
		‚Ä¢ Add to S all minimal generalizations h of s such that
			‚Äì h consistent with d
			‚Äì Some member of G is more general than h (<-*)
		‚Ä¢ Remove from S any hypothesis that is more general than another hypothesis in S
If d is a negative example:
	Remove from S any hypothesis that is inconsistent* with d [*h is 1]
	For each hypothesis g in G that is not consistent with d [specialize G]:
		‚Ä¢ remove g from G.
		‚Ä¢ Add to G all minimal specializations h of g such that
			‚Äì h consistent with d
			‚Äì Some member of S is more specific than h (<-* key point in the example later)
‚Ä¢ Remove from G any hypothesis that is less general than another hypothesis in G

Esempio Candidate Elimination: [Img. 11-2]

Inductive Bias and its role

‚Ä¢ Our hypothesis space is unable to represent a simple disjunctive target concept: 
	(Sky=Sunny) v (Sky=Cloudy)
x1 = <Sunny Warm Normal Strong Cool Change> +
x2 = <Cloudy Warm Normal Strong Cool Change> +
	S: {<?, Warm, Normal, Strong, Cool, Change>}
x3 = <Rainy, Warm, Normal, Strong, Cool, Change> -
	S: {}
Bias: We assume that the hypothesis space H contain the target concept c.
In other words that c can be described by a conjunction (by AND operator) of literals.

Unbiased Learner
‚Ä¢ Idea: Choose H that expresses every teachable concept, 
that means H is the set of all possible subsets of X: the power set P(X)
‚Ä¢ |X|=96, |P(X)|=2^96 ~ 10^28 distinct concepts
‚Ä¢ H = disjunctions, conjunctions, negations
	‚Äì e.g. <Sunny Warm Normal ? ? ?> v <? ? ? ? ? Change>
‚Ä¢ H surely contains the target concept.
‚Ä¢ What for generalitazion?

What are S and G in this case?
Assume positive examples (x1, x2, x3) andnegative examples (x4, x5)
S: {(x1 v x2 v x3)} 
G: {not(x4 v x5)}
The only examples that are unambiguolsy classified are the training examples themselves 
(H can represent them).
In other words in order to learn the target concept one would have to present 
every single instance in X as a training example.
Property: An unbiased learner is unable to generalize.
Proof: Each unobserved instance will be classified positive by precisely half the hypothesis in VS 
and negative by the other half (rejection).
Indeed:
Per ogni h consistent with xi (test), Esiste h‚Äô identical to h except h'(xi) <> h(xi),
h appartiene a VS -> h' appartiene a VS (they are identical on D)

Futility of Bias-Free Learning
‚Ä¢ A learner that makes no prior assumptions regarding the identity of the target concept 
has no rational basis for classifying any unseen instances.
‚Ä¢ (Restriction, preference) bias not only assumed for efficiency, it is needed for generalization
	‚Äì However, does no tell us (quantify) which one is the best solution for generalization
‚Ä¢ Trivial Example (TR = Training Set, TS = Test Set):
	X c(x)		H={x, not(x), 0, 1}
	TR 0 0 		VS={x,0}
	TS 1 ? 		‚Üí Can be 1 or 0... Unless you use all X as TR set.
‚Ä¢ Issue: characterize the bias for different learning approaches 

Inductive Bias
Consider:
‚Ä¢ Concept learning algorithm L
‚Ä¢ Instances X, target concept c
‚Ä¢ Training examples D_c={<x,c(x)>}
‚Ä¢ Let L(xi,D_c) denote the classification assigned to instance xi by L after training on D_c

Definition:
The inductive bias of L is any minimal set of assertions B such that:
for any target concept c and corresponding training data D_c:
	(Per ogni xi appartenente a X)[B && D_c && xi] |‚Äî L(xi, D_c)
Where A |‚Äî B means that A logically entails B (follows deductively from). 

Inductive Systems and Equivalent Deductive Systems:
	[Img. 11-3]

Three Learners with Different Biases
‚Ä¢ Rote learner (lookup table): Store examples, classify x if and only if 
it matches a previously observed example.
	‚Äì No inductive bias -> no generalization
‚Ä¢ Version space candidate elimination algorithm.
	‚Äì Bias: The hypothesis space contains the target concept (conj. of attributes) 
		|H|= 973 versus 10^28
‚Ä¢ Find-S
	‚Äì Bias: The hypothesis space contains the target concept 
	and all instances are negative instances unless the opposite is entailed by its other knowledge 
	(seen as positive examples).
	‚Äì In other words: we have a language bias due to the AND on the literals 
	plus the search bias due to the preference of the most specific hyphotesis

Other symbolic - rule based approaches
Overcome the conjunctive restriction toward flexible models:
Many approaches in ML, e.g.:
‚Ä¢ Decision trees
‚Ä¢ Genetic Algorithms:
	‚Äì encode each rule set as a bit string and uses genetic search operator to explore this H
‚Ä¢ Inductive Logic Programming:
	‚Äì First-order logic rules that contain variables (much more expressive than propositional rules)
	‚Äì Automatic inferring Prolog programs from examples (collection of Horn clauses)

Linear models

Sintesi (ingredienti)
‚Ä¢ Dati di Allenamento
‚Ä¢ Spazio delle Ipotesi H,
	‚Äì costituisce l‚Äôinsieme delle funzioni che possono essere realizzate dal sistema di apprendimento;
	‚Äì si assume che la funzione da apprendere f possa essere rappresentata da una ipotesi h in H
	(selezione di h attraverso i dati di apprendimento)
	‚Äì o che almeno una ipotesi h in H sia simile a f (approssimazione);
‚Ä¢ Algoritmo di Ricerca nello Spazio delle Ipotesi, alg. di apprendimento
	‚Äì E.g. Adattamento dei parametri liberi del modello al task
‚Ä¢ NOTA: H non pu√≤ coincidere con l‚Äôinsieme di tutte le funzioni possibili e la ricerca essere esaustiva 
		-> Bias Induttivo

Regression
Tasks: regression
‚Ä¢ Process of estimating of a real-value function on the basis of finite set of noisy samples
	‚Äì known pairs(x , f(x) + random noise)

Linear models
‚Ä¢ How to use the model
‚Ä¢ Build (learn) the model (a simple regression model)
	- i.e how to find the w values of the linear model in a ¬´systematic¬ª way

Univariate Linear Regression
‚Ä¢ Univariate case, simple linear regression:
	- We start with 1 input variable x, 1 output variable y
	- We assume a model h_w(x) expressed as out = w1x + w0
		-> where w are real-valued coefficients/free parameters (weights)
	- Fitting the data by a ‚Äústraight line‚Äù

Task and Model (an example)
‚Ä¢ Find h (linear model) that best fit data (from observed data set of x and y values)
‚Ä¢ Assuming that the given variable (y) is (linearly) related to another variable (x) or variables 
by y = w1x + w0 + noise, where w are the free par. and noise is the error in measuring the targets 
(with normal distribution)
‚Ä¢ we build a model (finding w values) to predict/estimate the price (y) 
of points for other (unobserved) x values

Use it (as ML predictive tool)!
‚Ä¢ Predict/estimate the price (y) of points for other (unobserved) x values

Build it: Learning via LMS (premise)
‚Ä¢ We have to find the values of the parameters w (w1 and w0 in the univariate case) 
in order to minimize the model output error (to make a good fitting)
‚Ä¢ Infinite hypthiesis space (continuous w values) but we have nice solution from classical math 
	‚Äì Surprisingly we can ‚Äúlearn‚Äù by this basic tool
	‚Äì Although simple it includes many relevant concept of modern ML 
	and it is a basis of evoluted methods in the field
‚Ä¢ Let us define a loss/error function and use the Least Mean Square (LMS) approach

Build it: Learning via LMS 
‚Ä¢ Learn ‚Üí find w such that minimize error/empirical loss 
(best data fitting ‚Äì on the training set with l examples)
‚Ä¢ Given a set of l training examples (xp, yp) p = 1...l
‚Ä¢ Find: h_w(x) in the form w1x + w0 (hence the values of w) 
that minimizes the expected loss on the training data.
‚Ä¢ For the loss we use the square of errors:
	- Least (Mean) Square: Find w to minimize the residual sum of squares [argminw Error(w) in L_2]:
		[Img. 12-1]

Why LMS to find the best h?
	[Img. 12-2]
‚Ä¢ The method of least squares is a standard approach to the approximate solution of over-determined systems,
i.e., sets of equations in which there are more equations than unknowns. 

How to solve?
	[Img. 12-3]

Spazi continui - gradient
‚Ä¢ Se la f √® continua e differenziabile, e.g. quadratica rispetto ad x
‚Ä¢ Il minimo o massimo si pu√≤ cercare utilizzando il gradiente, 
che restituisce la direzione di massima pendenza nel punto
‚Ä¢ Hill climbing iterativo: quantifica lo spostamento, senza cercarlo tra gli infiniti possibili successori

Nota generale: non sempre √® necessario il min/max assoluto: caso del ML

Gradient descent (local search)
‚Ä¢ iterative algorithm
‚Ä¢ Gradient = ascent direction: we can move toward the minimum with a gradient descent 
		(Deltaw= - gradient of E(w))
-> Local search: begins with initial weight vector. 
Modifies it iteratively to decrease up to minimize the error function (steepest descent)

w_new = w + eta * Deltaw 
Where eta is a constant (step size), called learning rate

Gradient descent: intuitive motivations
‚Ä¢ This is an ‚Äúerror correction‚Äù rule (call it delta rule) 
that changes the w proportionally to the error (target-output): 
	‚Äì (target y ‚Äì output) = err = 0 -> no correction
	‚Äì output > target -> (y - h) < 0 (output is too high)
		-> Deltaw0 negative ‚Üí reduce w0 and
		if (input x > 0) Deltaw1 negative -> reduce w1 
		else increase w1
	‚Äì output < target ‚Üí (y - h) > 0 (output is too low)... [exercise]

Gradient descent: final discussion
Gradient descent approach is a simple and effective local search approach to LMS solution and:
	‚Ä¢ it allows us to search through an infinite hypothesis space!
	‚Ä¢ it can be easily always applied for continues H and differentiable loss: 
	NOT ONLY to linear models! (e.g. Neural Networks and deep learning models)
	‚Ä¢ Efficient? Many improvement are possible, e.g. Newton's methods, Conjugate Gradient...

Linear models
‚Ä¢ Extension to l data
‚Ä¢ Extension to input vectors
‚Ä¢ Summary of the LMS
‚Ä¢ The gradient descent training algorithm

For l patterns:
	[Img. 12-4]
‚Ä¢ We can update w after (repeating) an ‚Äúepoch‚Äù of l training data -> Batch algorithm 
‚Ä¢ Or we can update w after each pattern p ‚Üí On-line algorithm (stochastic gradient descent)
	‚Äì it can be the faster, but needs smaller step

Multidimensional input (input vectors x): examples
‚Ä¢ This is the standard case, using 2 up to hundreds of input variables/features:
	x = [x1, x2, ..., xn] (The input pattern is a vector)
‚Ä¢ many possibilities in a wide range of applicative fields

‚Ä¢ w0 is the intercept, threshold, bias, offset...

Geometrical view: hyperplane
	[Img. 12-5]

Summary: Input dimension n, l patterns
‚Ä¢ Given a set of l training examples (xp, yp)
‚Ä¢ Find: The weight vector w that minimizes the expected loss on the training data
	[Img. 12-6]

Summary: Gradient descent algorithm
A simple algorithm:
1) Start with weight vector w_initial (small), fix eta (0 < eta < 1).
2) Compute Deltaw= ‚Äì ‚Äúgradient of E(w)‚Äù (i.e. for each wi)
3) Compute w_new = w + eta*Deltaw 	(i.e. for each wi) 
where eta is the ‚Äústep size‚Äù (learning rate) parameter
4) Repeat (2) until convergence or E(w) is "sufficiently small"

‚ñ™ Deltaw/l: least mean squares
‚Ä¢ Batch versions (Deltaw after each ‚Äúepoch‚Äù of l data): as previous slides
‚Ä¢ Stochastic/on-line version: upgrade w for each pattern p (without waiting the total sum over l)
‚Ä¢ eta -> learning rate: speed/stability trade-off, can be (gradually) decreased to zero 
(guarantees convergence, avoiding oscillation around the min.)

Advantages of linear models
‚Ä¢ If it works well it is a ‚Äúwonderful‚Äù model
	‚Äì Very simple
	‚Äì All the information on data is in w
	‚Äì Easy to be interpreted: everyday practice in medicine, biology, chemistry, economy...
	‚Äì Noisy data are allowed
	- Statisticians are happy (nice properties)
	- Linear phenomena -> a dream for science: ideal to make a ‚Äúnatural law‚Äù
‚Ä¢ A baseline for learning (first: is it a linear problem?)
‚Ä¢ It is used/included in more complex models

Linear models
‚Ä¢ Limitations
‚Ä¢ Linear Basis Expansion
‚Ä¢ Regularization

Moving toward non-linear relationships
Note that in hw(x) = w1x + w0
‚Ä¢ As Statistical Parametric models: "linear" does not refer to this straight line, 
but rather to the way in which the regression coefficients w occur in the regression equation
‚Ä¢ Hence, we can use also transformed inputs, such are x, x^2, x^3, x^4...
with non-linear relationship inputs and output  holding the learning machinery 
(Least Square solution) developed so far

Example
‚Ä¢ The result of fitting a quadratic function, M=2, through a set of data points.
‚Ä¢ In linear least squares the function need not be linear in the argument (input variables) 
but only in the parameters (w) that are determined to give the best fit.

Generalization - LBE (shown for regression)
‚Ä¢ Basis transformation -> linear basis expansion:
	[Img. 12-7]
‚Ä¢ Augment the input vector with additional variables which are transformations of x 
according to a function phi (phi_k: R^n -> R)
‚Ä¢ E.g.:
	‚Äì Polynomial representation of x: phi(x)=xj^2 or phi(x)= xjxi, or...
	‚Äì Non-linear transformation of single inputs: phi(x)=log(xj), phi(x)= root(xj)...
	‚Äì Non-linear transformation of multiple input: phi(x)= ||x||
	‚Äì Splines...
‚Ä¢ Number parameters K > n (before it was n)
‚Ä¢ The model is linear in the parameters (also in phi, not in x): 
we can use the same learning alg. as before!

Basis Expansion examples:
	[Img. 12-8]

Basis Expansion criticism
‚Ä¢ Which phi? Toward the so called ‚Äúdictionary‚Äù approaches
‚Ä¢ PROS: 
	+ It is more expressive: it can model more complicated relationships (than linear).
‚Ä¢ CONS: 
	- With large basis of functions, we require methods for controlling the complexity of the model

1st Order Polynomial (linear model): 
	- M = 1
	- too poor solution (underfitting)
3rd Order Polynomial: 
	- M = 3
	- more flexibility is useful!
9th Order Polynomial:
	- M = 9
	- excessive flexibility! E(w) = 0 on training data, but error on test set
	- too complex model: fit the noise
	- poor representation of the true function (overfitting)

Complexity tradeoff
‚Ä¢ Too simple model:
	‚Äì Doesn‚Äôt fit the data well
	‚Äì Biased solution
	- UNDERFITTING
‚Ä¢ Too complex model:
	‚Äì Highly sensitive to slight perturbations of the data
	‚Äì High variance solution (Bias-variance)
	- OVERFITTING
‚Ä¢ Want to choose regularization to balance out bias and variance
	-> Trough control of model complexity (seen as flexibility)

Toward Regularization
‚Ä¢ It can control overfitting by penalizing ‚Äúcomplex‚Äù functions (large weights) 
(i.e. toward coefficient shrinkage) while maintain the flexibility of the hypothesis space
‚Ä¢ Occam (Ockham) razor:
	‚ÄúThe simplest explanation is more likely the correct one‚Äù 
	or ‚Äúprefer the simplest hypothesis that fits the data‚Äù
‚Ä¢ This fundamental concept in ML will be found again and we will ‚Äúrationalize‚Äù it at the end 
	‚Äì Whereas complexity is not for the computational cost 
	but a measure of the flexibility of the model to fit the data
‚Ä¢ Now let us introduce an approach for linear models: regularization by ridge regression

Regularization by Ridge Regression
‚Ä¢ Ridge regression/Tikhonov regularization: smoothed models
	‚Äì possible to add constraints to the sum of value of |wj| favouring "sparse" models 
	e.g. with less terms due to weights wj = 0 (or close to 0) (it means a less complex model)
Formula:
	[Img. 12-9]
‚Äì Effect: weight decay (basically add 2*Lambdaw to the gradient of the Loss)
E.g. with zero gradient, it decreases the value of each w with a fraction of the old w

Regularization by Ridge Regression: discussion
‚Ä¢ General applicability (not only for polynomials)
	‚Äì e.g. we can control model complexity just using lambda, without not knowing M as for polynomials,
	or even when the model complexity is not known
‚Ä¢ Note the balancing (trade-off) between the two terms:
	‚Äì Small error data term (minimize just the training error) is not enough for us: 
	we want also to control the complexity of the model to control the overfitting, 
	hence we introduce the second term in the minimization.
	‚Äì On the other side, we can exceed because to much weight to the second term (high lambda value)
	leads to focus the minimization only or mostly on the regularization, hence the data error 
	(first term) could grow too much, i.e. moving to underfitting
	‚Äì The trade-off is ruled by the value of lambda (regularization coefficient)

Too low lambda (lambda = 0) -> OVERFITTING
Too high lambda (lambda = 1) -> UNDERFITTING

Limitations of Fixed Basis Functions
‚Ä¢ Having basis function along each dimension of a D-dim. input space 
requires combinatorial number of functions
	-> the curse of dimensionality
	‚ñ™ e.g. general polynomials of order 3 use all combination of input variables 
	due to products x1*x2, x2*x3, ..., x1*x2*x3 etc. -> D^3 (approximationas D increases)
‚Ä¢ Phi are fixed before observing training data
	- In other models, we can get away with fewer basis functions, by choosing these 
	using the training data: phi (in a hidden computational layer) depends on w 
	and the model is not-linear in the parameters. E.g. Neural Networks
	- In other models the computation of the new embedding space (input transformation) 
	is made implicitly through kernel functions and controlling the complexity of the model. E.g. SVM

Linear model - Classification
We reuse the linear model
‚Ä¢ The same models (used for regression) can be used for classification: 
categorical targets, e.g. 0/1 or -1/+1.
- In this case we use a hyperplane (wx) assuming negative or positive values
- We exploit such models to decide if a point x belongs to positive or negative zone of the hyperplane 
(to classify it)
- So we want to set w (by learning) so that we get good classification accuracy

Geometrical view: hyperplane and classifier
	[Img. 12-10, 12-11]

Classification by linear decision boundary
The classification may be viewed as the allocation of the input space in decision regions (e.g. 0/1)
Example: linear separator on 2-dim instance space x = (x1, x2) in R^2, f(x) = 0/1 (or -1/+1) 

h(x) = { 1 if wx + w0 >= 0	[0, 1] outputs
	 0 otherwise }
h(x) = sign(wx + w0)		[-1, +1] outputs
	-> Linear threshold unit (LTU)

Threshold (bias w0)
Note that, given the bias w0, in the LTU saying:
	h(x) = w^Tx + w0 >= 0 
is equivalent to saying:
	h(x) = w^Tx >= -w0
with ‚Äìùë§0 as the ¬´threshold¬ª value

‚Ä¢ The two forms identify the same positive zone of the classifier
‚Ä¢ The second one emphasizes the role of the bias as a threshold value 
to ‚Äúactivate‚Äù the +1 output of the classifier.

Use it: example
	[Img. 12-12]

Learning problem for linear classifiers: 
	[Img. 12-13]

DeltaW as Error Correction Learning rule
If misclassified (because target is +1) 
	-> High positive delta for w1 and w3 
		-> increase them proportionally (with eta) to the delta, 
	hence a positive value in this case [error correction rule]

Summarizing
‚Ä¢ Model trained (on tr set) with LS (LMS) on wx 
by the simple gradient descent algorithm used for linear regression
‚Ä¢ Model used for classification applying the threshold function h(x) = sign(wx)
‚Ä¢ The error can be computed as classification error or number of misclassified patterns 
(not only by the Mean Square Error)
‚Ä¢ ACCURACY = mean of correctly classified = (l-num_err)/l

Learning Algorithm
‚Ä¢ The same as for regression: see ‚ÄúGradient descent algorithm‚Äù
‚Ä¢ Also the linear basis expansion and Tikhonov can be applied as well
‚Ä¢ Note that (learning algorithm):
	‚Äì It converges asymptotically to the MinLS
	‚Äì Not necessarily to the min number of classification errors
‚Ä¢ [If w are not changed for correct outputs (hw(xp) = dp) we obtain the update rule for the perceptron]

Example: Conjunctions
‚Ä¢ We can represent conjunctions by the linear models, e.g.:
4 var: x1 && x2 && x4 <-> y
‚Ä¢ 1 x1 + 1 x2 + 0 x3 + 1 x4 > 2
‚Ä¢ 1 x1 + 1 x2 + 0 x3 + 1 x4 >= 2.5

2 var: x1 && x2 <-> y
‚Ä¢ 1 x1 + 1 x2 > 1
‚Ä¢ 1 x1 + 1 x2 >= 1.5

Limitations
‚Ä¢ In geometry, two set of points in a two-dimensional plot are linearly separable 
when the two sets of points can be completely separated by a single line
‚Ä¢ In general, two groups are linearly separable in n-dimensional space 
if they can be separated by an (n-1)-dimensional hyperplane.
‚Ä¢ The linear decision boundary can provide exact solutions only for linearly separable sets of points

Non linear decision boundary: Examples
	[Img. 12-14]

Other learner models for classification
‚Ä¢ Linear Discriminant Analysis (also multi-class)
‚Ä¢ Logistic regression
	‚Äì P(y|x) starting from modeling the class density as a know density
	‚Äì Soft threshold (continuous, differentiable) with logistic function (instead of 0/1 hard threshold)
‚Ä¢ Neural networks (NN) and SVM mentioned before: flexible models including non-linear approximation 
for both regression and classification.
	‚Äì NN use many units (similar to LTU) within different layers
	‚Äì Feature representation learning in each layer (deep learning concept)
	‚Äì Gradient descent approach for learning

Conclusions on Linear Models
‚Ä¢ Basic well-founded approach for both regression and classification
	‚Äì Very compact way to represent knowledge
	‚Äì But with a strong assumption on the relationship among data
‚Ä¢ An iterative error correction (LMS) algorithm that search continuous hypothesis spaces
	‚Äì (that is the basis for many other ML approaches)
‚Ä¢ A view of limitations of linear approaches and of the needs for more flexible ML models and their issues:
	‚Äì An extension of linear model for non-linear tasks
	‚Äì An introduction to the control of complexity (regularization)

Introduction to Decision Trees Learning

Overcome the conjunctive restriction toward flexible models:
[in the class of symbolic - rule based approaches]
‚Ä¢ Decision trees (a popular approach in ML/DM)

Decision trees: expressive power
‚Ä¢ Decision trees represent a disjunction of conjunctions of constraints on the value of attributes:

(Outlook = Sunny || Humidity = Normal) ||
(Outlook = Overcast) ||
(Outlook = Rain && Wind = Weak)

Or if-then-else rules.
H of D.T. capable of expressing any finite discrete-valued function (propositional)

Top-down induction of Decision Trees
‚Ä¢ ID3 is a basic algorithm for learning DT's
‚Ä¢ Given a training set of examples, the algorithms for building DT 
performs search in the space of decision trees
‚Ä¢ The construction of the tree is top-down. The algorithm performs a greedy search.
‚Ä¢ The fundamental question is ‚Äúwhich attribute should be tested next‚Äù? 
Which question gives us more information gain?‚Äù
‚Ä¢ Select the best attribute
‚Ä¢ A descendent node is then created for each possible value of this attribute 
and examples are partitioned according to this value
‚Ä¢ The process is repeated for each successor node until all the examples 
are classified correctly or there are no attributes left

ID3: algorithm
ID3(X, T, Attrs) 
			X: training examples
			T: target attribute (e.g. PlayTennis)
			Attrs: other attributes, initially all attributes
Create Root node
If all X's are +, return Root with class +
If all X's are ‚Äì, return Root with class ‚Äì
If Attrs is empty return Root with class most common value of T in X
else
	A <- best attribute; decision attribute for Root <- A
	For each possible value vi of A:
		- add a new branch below Root, for test A = vi
		- Xi <- subset of X with A = vi
		- If (Xi is empty) then 
			add a new leaf with class the most common value of T in X
		else 
			add the subtree generated by ID3(Xi, T, Attrs ‚àí {A})
return Root

Selecting the best attribute: entropy
‚Ä¢ We use the notion of entropy, commonly used in information theory
‚Ä¢ Entropy measures the impurity of a collection of examples. 
It depends from the distribution of the random variable p.
	‚Äì S is a collection of training examples
	‚Äì p+ the proportion of positive examples in S
	‚Äì p‚Äì the proportion of negative examples in S

Entropy (S) = ‚Äì p+ * log2(p+) ‚Äì p‚Äì * log2(p‚Äì)	[assume 0 log2(0) = 0]

Entropy ([14+, 0‚Äì]) = ‚Äì 14/14 log2(14/14) ‚Äì 0 log2(0) = 0
Entropy ([9+, 5‚Äì]) = ‚Äì 9/14 log2(9/14) ‚Äì 5/14 log2(5/14) = 0,94
Entropy ([7+, 7‚Äì ]) = ‚Äì 7/14 log2(7/14) ‚Äì 7/14 log2(7/14) = 1/2 + 1/2 = 1
[High impurity, low homogeneity -> no good]

Note: 0 <= p <= 1, 0 <= entropy <= 1

All +1 or All -1 -> 0 entropy, ‚Äúno information‚Äù . Max for S with 1/2 + and 1/2 - examples

Selecting the best attribute: information gain definition
‚Ä¢ Information gain is the expected reduction in entropy caused by partitioning the examples on an attribute.
‚Ä¢ Expected reduction in entropy knowing A 

Def. Gain:
	[Img. 13-3]

‚Ä¢ The higher the information gain the more effective the attribute in classifying training data 
(higher variation in the homogeneity of the class distribution over the sub sets, 
max separation of classes).
‚Ä¢ Homogeneous, as [14+, 0‚Äì] or [0+,7-] allow us ‚Äúclear‚Äù classification

Selecting the best attribute: information gain: why?
‚Ä¢ Entropy to measure the homogeneity (indeed impurity) of the class of the subset of examples, hence:
‚Ä¢ Select A that maximize Gain
	After splitting, lower values (more homogeneous targets) in each subsets -> higher Gain 
‚Ä¢ The aim is to separate the examples on the basis of the target, finding the attribute 
that discriminates examples that belong to different target classes
	‚Äì e.g. all the ‚Äì on the left, all the + on the right

Select the best attribute: example
	[Img. 13-4]

Going on 
‚Äì The information gain is computed for the all the attributes and the one with highest inf. gain 
is selected as the first decision node (e.g. outlook)
‚Äì The training data are partioned for values of outlook (and associated to following nodes)
‚Äì If entropy is not zero in a set, the tree continues to grow there 
(with such data and remaining attributes).
‚Äì Obtaining the flowing DT

Problems with information gain
‚Ä¢ The information gain favors attributes with many possible values.
‚Ä¢ Consider the attribute Date in the PlayTennis example.
	‚Äì Date has the maximum information gain: Every day correspond to a different subset which is pure 
		-> 0 entropy
	‚Äì Perfect fit (separation) of TR data
	‚Äì But it is not significant: not useful for unseen instances!
‚Ä¢ How to avoid generation of many small subsets?

An alternative measure: gain ratio

GainRatio(S, A) = Gain(S, A) / SplitInformation(S,A)
where SplitInformation(S,A) = [Img. 13-5]
‚Ä¢ SplitInformation measures the entropy of S with respect to the values of A. 
The more uniformly dispersed the data the higher it is.

‚Ä¢ GainRatio penalizes attributes that split examples in many small classes such as Date. 
Let |S| = n, Date splits examples in n subsets.
	SplitInformation(S, Date) = ‚àí[(1/n) log2(1/n))+ ... + (1/n) log2(1/n)] = ‚àílog2(1/n) = log2(n)
	which is > 1 for n > 2 -> we reduce the GainRatio
‚Ä¢ Compare with an A which splits data in two even classes (with (n/2)/n= 1/2):
	SplitInformation(S, A)= ‚àí [(1/2) log2(1/2) + (1/2) log2(1/2)]= ‚àí[‚àí(1/2) ‚àí(1/2)] = 1

Adjusting gain ratio
‚Ä¢ Problem: SplitInformation(S, A) can be zero or very small when |Si| ‚âà |S| for some value i [log 1 = 0]
	‚Äì E.g. An extreme case with |S1| = 0, |S2| = n
	‚Äì SplitInformation = - [0/n * log 0/n + n/n * log n/n] = - 0 - 0 = 0
	‚Äì E.g. an attribute with the same value for the examples, not good indeed!
‚Ä¢ To mitigate this effect, the following heuristics has been used:
	1. compute Gain for each attribute
	2. apply GainRatio only to attributes with Gain above average
‚Ä¢ Other measures have been proposed...

Hp Space Search in DT learning

Hp space search by ID3, Search (hill-climbing) through the space of possible DT 
from simplest to increasingly complex

WRT to previous alg. (Cand. Elimin.):
‚ñ™ The hypotheses space is complete (represents all discrete-valued functions)
‚ñ™ The search maintains a single current hypothesis
‚ñ™ No backtracking; no guarantee of optimality (local optima)
‚ñ™ It uses all the available examples (not incremental)
‚ñ™ May terminate earlier, accepting noisy classes

Inductive bias in DT learning
‚ñ™ What is the inductive bias of DT learning?
	1. Shorter trees are preferred over longer trees
		-> Due to simple-to-complex search.
		Not enough. This is the bias exhibited by a simple breadth first algorithm 
		generating all DT's and selecting the shorter consistent one
	2. Prefer trees that place high information gain attributes close to the root

‚ñ™ Note: DT's are not limited in representing all possible functions.
‚ñ™ The restriction is not over the Hp space, it is on the search strategy.

Two kinds of Biases
‚ñ™ Preference or search biases (due to the search strategy)
	- ID3 searches a complete hypotheses space; the search strategy is incomplete
‚ñ™ Restriction or language biases (due to the set of hypotheses expressible or considered)
	- Candidate-Elimination searches an incomplete hypotheses space; 
	the search strategy is complete (all the consistent hp)
‚ñ™ A combination of biases in learning a linear models.

Why the search bias can be preferred over the language bias?
‚ñ™ In ML typically use flexible approaches (universal capability of the models), 
without excluding a priori the unknown target function
‚ñ™ Of course, flexible -> overfitting issue!

Prefer shorter hypotheses: Occam's razor
‚ñ™ Why prefer shorter hypotheses?
	- Again, The Occam (Ockham) razor
	‚ÄúThe simplest explanation is more likely the correct one‚Äù 
	Or ‚Äúprefer the simplest hypothesis that fits the data‚Äù
	- Lex parsimoniae ("law of parsimony", "law of economy", or "law of succinctness")
‚ñ™ The term razor refers to the act of shaving away unnecessary assumptions 
to get to the simplest explanation.
‚Ä¢ Remember the ‚Äúcontrol of model complexity‚Äù by regularization for linear models
‚Ä¢ This fundamental concept in ML will be found again and we will ‚Äúrationalize‚Äù it at the end

Issues in decision trees learning
‚ñ™ Overfitting
	- Reduced error pruning
	- Rule post-pruning
‚ñ™ Extensions (specific for DT):
	- Alternative measures for selecting attributes [done]
	- Continuous valued attributes
	- Handling training examples with missing attribute values
	- Handling attributes with different costs
	- Improving computational efficiency

Overfitting: definition
‚Ä¢ Building trees that ‚Äúadapt too much‚Äù to the training examples may lead to ‚Äúoverfitting‚Äù.
‚Ä¢ Consider error of hypothesis h over:
	‚Äì training data: errorD(h)			empirical error
	‚Äì entire distribution X of data: errorX(h)	expected or true error

‚Ä¢ Hypothesis h overfits training data if there is an alternative hypothesis h' appartenente ad H such that:
errorD(h) < errorD(h‚Äô) and 
errorX(h‚Äô) < errorX(h)
i.e. h‚Äô behaves worse on TR data, better over unseen data

Flexible approaches can easily encounter overfitting.

‚ÄúAvoiding‚Äù* overfitting in DT
[‚ÄúAvoiding‚Äù is not correct, we can see the methods to mitigate the overfitting, 
but we have still to control it by a conscious management]
Two strategies:
1. Stop growing the tree earlier, before perfect classification
2. Allow the tree to overfit the data, and then post-prune the tree

How to assess the effect?
‚ñ™ Training and validation set
	- split the training set in two parts (training and validation) 
	and use validation set to assess the utility of post-pruning
‚ñ™ Other approaches:
	- Use a statistical test to estimate effect of expanding or pruning
	- Minimum description length principle: uses a measure of complexity of encoding the DT 
	and the examples, and halt growing the tree when this encoding size is minimal

Reduced-error pruning
‚ñ™ Each node is a candidate for pruning
‚ñ™ Pruning consists in removing a subtree rooted in a node: the node becomes a leaf 
and is assigned the most common classification
‚ñ™ Nodes are removed only if the resulting tree performs no worse on the validation set.
‚ñ™ Nodes are pruned iteratively: at each iteration the node whose removal most increases accuracy 
on the validation set is pruned.
‚ñ™ Pruning stops when no pruning increases accuracy

Rule post-pruning
1. Create the decision tree from the training set
2. Convert the tree into an equivalent set of rules
	‚Äì Each path corresponds to a rule
	‚Äì Each node along a path corresponds to a pre-condition
	‚Äì Each leaf classification to the post-condition
	e.g. (Outlook=Sunny) && (Humidity=High) -> (PlayTennis=No)
3. Prune (generalize) each rule by removing those preconditions whose removal improves accuracy...
	‚Äì ...over validation set
	‚Äì ...over training with a pessimistic, statistically inspired, measure (heuristic)
4. Sort the rules in estimated order of accuracy, and consider them in sequence 
when classifying new instances

Why converting to rules? 
‚ñ™ Each distinct path produces a different rule: a condition removal 
may be based on a local (contextual) criterion.
‚ñ™ Pruning of preconditions is rule specific, node pruning is global and affects all the rules
‚ñ™ Converting to rules improves readability for humans

Dealing with continuous-valued attributes
‚ñ™ So far discrete values for attributes and for outcome.
‚ñ™ Given a continuous-valued attribute A, dynamically create a new attribute Ac
	Ac = True if A < c, False otherwise
‚ñ™ How to determine threshold value c?

Example. Temperature in the PlayTennis example
- Sort the examples according to Temperature
- Determine candidate thresholds by averaging consecutive values where there is a change in classification
- Evaluate candidate thresholds (attributes) according to information gain. 
The new attribute competes with the other ones

Handling incomplete training data (imputation)
‚ñ™ How to cope with the problem that the value of some attribute may be missing?
	- Example: Blood-Test-Result in a medical diagnosis problem
‚ñ™ The strategy: use other examples to guess attribute (within training data at a given node)
	1. Most common. Assign the value that is most common 
	among all the training examples at the node/those in the same class
	2. Assign a probability to each value, based on frequencies, and assign values to missing attribute,
	according to this probability distribution (adding more examples weighted by prob.)
‚ñ™ Missing values in new instances to be classified are treated accordingly (weighting), 
and the most probable classification is chosen

Handling attributes with different costs
‚Ä¢ Instance attributes may have an associated cost:
we would prefer decision trees that use low-cost attributes
‚Ä¢ ID3 can be modified to take into account costs:
1. Tan and Schlimmer
	Gain^2(S, A)/Cost(S, A)
2. Nunez
2^(Gain(S, A)) ‚àí 1/(Cost(A) + 1)^w	w appartenente a [0,1]

A geometrical view 
‚Ä¢ Decision boundaries that can be produced by a DT:
Decision Trees divide the input space into axis-parallel rectangles 
and label each rectangle with one of the K classes (leaf of the tree)
	[Img. 13-6]

Conclusions
‚Ä¢ DT is a popular approach for classification in a discrete number of classes.
	‚Äì Expressive hypotheses space in the area of propositional - rule based approaches.
	‚Äì Robust to noisy data. Deal with Missing attribute values.
	‚Äì Easy to be understood (explicit if-then rules, unless they are an huge amount)
	‚Äì Many extensions to the basic scheme...
‚Ä¢ Language and search biases: ID3 searches a complete hypothesis space, with a greedy incomplete strategy
‚Ä¢ Flexible approach: Overfitting is an important issue, 
tackled by post-pruning and generalization of induced rules

Introduction to Validation and Theoretical Issues

Obiettivi (overview) 
Questioni fondamentali del ML:
evaluate generalization capabilities (of your hp)
	‚Äì ruolo essenziale della validazione
	‚Äì cenni (dell‚Äôesistenza) di fondamenti teorici (supporto al significato del ML)

‚Ä¢ Aspetto sia teorico che pratico per un uso consapevole del ML

ML issues
Quando un modello di ML √® un buon modello?
Usare il ML versus usare bene il ML

Machine Learning: generalization
‚Ä¢ Learning: search for a good function in a function space from known data
‚Ä¢ Good w.r.t. generalization error: it measures how accurately the model predicts 
over novel samples of data (low error, high accuracy and vice versa) 

‚Ä¢ Inductive learning hypothesis
	‚Äì Any h that approximates f well on training examples 
	will also approximate f well on new (unseen) instances x (?)
‚Ä¢ Punto centrale, ma come obiettivo del ML:
	‚Äì Teoria che supporta in che condizioni ci√≤ si verifca
	‚Äì Guida la scelta del ‚Äúbest model‚Äù 
	(tra modelli diversi o configurazioni diverse: iperparametri, livello di training...)
	‚Äì Va verificato nelle applicazioni

‚Ä¢ Generalization: crucial point of ML!
‚Ä¢ Learning phase (training, fitting): build the model from know data ‚Äì training data (and bias)
‚Ä¢ Predictive phase (test): apply to new example
	- we take the input x and we compute the response by the model; 
	we compare with its target that the model has never seen: 
	evaluation of the predictive hypothesis, i.e. of the generalization capability

Note: performance in ML = predictive accuracy 
estimated by the error computed on the (Hold out) Test Set
‚Ä¢ Overfitting: A learner overfits the data if it outputs a hypothesis h(.) in H having true error eps
and empirical (TR) error E, but there is another h‚Äô(.) in H having E‚Äô > E and eps‚Äô < eps

Premise: which measure?
Recap:
To evaluate typically we measure...
‚Ä¢ For classification: MSE for the loss, accuracy or mean error rate for the outcome
	‚Äì but also precision, recall or specificity, sensitivity 
	(accounting for False Positive, False Negative)...
‚Ä¢ For regression: MSE, Root MSE (S), Mean Absolute Error, Max Absolute Error...
	‚Äì but also statistics measures such R (correlation coefficient/index ), etc.
‚Ä¢ Of course high error <-> low accuracy (both for training, test, etc.)
	‚Äì E.g. poor fitting with high training error...
	‚Äì E.g. poor generalization with high test error...

Validation: Two aims
‚Ä¢ Model selection: estimating the performance (generalization error) of different learning models 
in order to choose the best one (to generalize).
	‚Äì this includes searching for the best hyper-parameters of your model
	(e.g. polynomial order, lambda of ridge regression...)
	- It returns a model
‚Ä¢ Model assessment: having chosen a final model, estimating/evaluating its prediction error/risk
(generalization error) on new test data (measure of the quality/performance of the ultimately chosen model).
	- It return an estimation

Hold out
‚Ä¢ If data set size is sufficient: e.g. 50% TR, 25% VL, 25% TS disjoint sets

‚Ä¢ TR: Training set is used to fit 							[training]
‚Ä¢ VL: Validation set (or selection set) can be used to select the best models
(e.g. hyper-parameters tuning) 								[model selection]
‚Ä¢ TR+VL sometimes are jointly called development/design set, i.e. used to build the final model
‚Ä¢ TS: Test set is used for estimation of generalization error (of the final model) 	[model assessment]

Notes:
1) the estimation made for model selection (on VL set) is for model selection purpose, 
it is not a good estimation for the assessment phase/risk test.
2) Test set cannot be used for model selection (or call it validation set)

Test or model selection?
‚Ä¢ What if test set is used in a (repeated) design cycle?
	‚Äì We are making model selection and not reliable assessment 
	(estimation of expected generalization error)
		-> and we wouldn't be able to do that on future examples
		-> Blind test set concept (e.g. for ML competitions)
		-> Image an exam exercise: if you see the solutions it is not a test!
	‚Äì In that case, used test set error provides an overoptimistic evaluation of the true test error 
	(we will see how easy is to obtain very high classification accuracy over random task 
	even using the test set only implicitly)

Gold rule: Keep separation between goals and use separate sets
(TR for training, VL for model selection, TS for risk estimation)
TR = training set, VL = validation set, TS = test set

TR/VL/TS by a simple schema:
	[Img. 14-2]

A simple meta-algorithm
‚Ä¢ Separate TR (training), VL (validation) and TS (test) sets
‚Ä¢ Search best h_w,Œª() changing the model hyper-parameters Œª 
[e.g. the polynomial order, the lambda for ridge regression]:
For each different values of Œª (grid search):
	‚Äì Search best h_w,Œª() that minimizes error/empirical loss (fitting the TR set) 
	finding the best w parameters, where best = minimum error on TR set [argminw Loss(w) in L2]
‚Ä¢ Select the best best h_w,Œª(): where best = minimum error on the VL set
‚Ä¢ (Optional: Now it is also possible to fit h_w,Œª(x) on TR+VL with best Œª model)
‚Ä¢ Evaluate the final h_w,Œª(x) on the TS

Search on a grid (e.g. with 2 hyper-parameters)
‚Ä¢ Find hyper-parameters value (i.e. parameters that are not directly learnt, 
which are not modified by training)
‚Ä¢ Search best hyper-parameter can be a <<FOR>> over a grid of candidate values. 
For each trained model h_w,Œª compute the results (accuracy) on the VL set. 
Then take the one with the minimum error or the max accuracy.

Controesempio (separare VL and TS)
	‚Äì 20-30 esempi, 1000 variabili di input random,
	‚Äì random target 0/1
	‚Äì Scelgo 1 modello con una sola variabile/feature che indovina 'per caso' 
	al 99% su qualsiasi split successivo in training, validation e test set.
Perfect result (a model with accuracy 99% )? What is wrong?
99% non √® una buona stima dell‚Äô errore di test (quella corretta e' 50%)

1. Errore stimato su training o validation per model selection NON √® utile per stima del rischio!
2. Usare tutto il data set per feature/model selection lede la correttezza della stima
(risultati biased ‚Äì ¬´Feature Selection bias¬ª).
	‚Äì Test set √® stato usato implicitamente all‚Äôinizio).
	‚Äì Test deve essere separato prima, prima di qualsiasi model selction (incluso selezione di features)
Un test set esterno fornisce invece la stima corretta del 50% (random coin result!).
√à la correttezza della stima che √® in giudizio, non la possibilit√† di risolvere il task!

Delicato confrontando metodi diversi e usando tecniche di K-fold cross-validation 
che in s√© non garantiscono correttezza della procedura di validazione.

Hold out and K-fold cross validation
Hold out CV can make insufficient use of data

K-fold Cross-Validation
‚Ä¢ Split the data set D into k mutually exclusive subsets D1, D2, ..., DK
‚Ä¢ Train the learning algorithm on D\Di and test it on Di
‚Ä¢ Summarize averaging all the Di results
‚Ä¢ It uses all the data for training and validation or testing
‚Ä¢ NOTE: This technique can be used both for the validation set or for the test set

Issues:
‚Ä¢ How many folds? 3-fold, 5-fold, 10-fold, ..., 1-leave-out
‚Ä¢ Often computationally very expensive
‚Ä¢ Combinable with validation set, double-K-fold CV...

An example of model selection and assessment (with K-fold CV)
‚Ä¢ Split data in TR and Test set (here simple hold-out or a K-fold CV)
‚Ä¢ [Model selection] Use K-fold CV (internal) over TR set, obtaining new TR e VL set in each folder, 
to find best hyper-parameters of your model (e.g. polynomial order, lambda of ridge regression...):
apply a grid-search with many possible values of the hyper-par.
	‚Äì i.e. for example a k-fold-CV for Œª = 0.1, a k-fold-CV CV for Œª = 0.01...
	and then take the best Œª (comparing the mean errors computed over the validation sets 
	obtained by the all the folds of each k-fold CV)
‚Ä¢ Train on the whole TR set the final model
‚Ä¢ [Model assessment] Evaluate it on the external Test set

Validation: summarizing
‚Ä¢ Stima Empirica: errore calcolato/stimato su (Hold out)
	‚Äì E.g. Hold out: training, validation set and test sets
	‚Äì K-fold cross validation (resampling)
‚Ä¢ Teoria: E.g. Statistical Learning Theory
	‚Äì sotto quali condizioni (matematiche) un modello √® capace di generalizzare (buona generalizazione)?

Typical behavior of learning
- Training and test errors are high: UNDERFITTING
	-> the model is too simple w.r.t. to the target function both for known data and new data
- Training error is low, test error is high: OVERFITTING
	-> the model is too complex: Fit the noise.

Toward SLT
Putting all together:
‚Ä¢ The generalization capability (measured as a risk or test error) of a model
	‚Äì with respect to the training error
	‚Äì overfitting and underfitting zones
‚Ä¢ The role of model complexity
‚Ä¢ The role of the number of data

‚Ä¢ Statistical Learning Theory (SLT): a general theory relating such topics

Formal Setting Statistical Learning Theory (SLT):
	[Img. 14-3]

Vapnik-Chervonenkis-dim and SLT: a general theory
‚Ä¢ Given the VC-dim (VC), a measure complexity of H (flexibility to fit data) 
(e.g. Num. of parameters for linear models/polynomials)
	- Can we use Remp to approximate R?

‚Ä¢ VC-bounds: [Img. 14-4]

‚Ä¢ First (basic) explanation:
‚Äì Œµ is a function directly proportional to VC (VC-dim), inversely proportional to l and delta.
‚Äì We know that Remp decreases using complex models (with high VC-dim)
‚Äì delta is the confidence, it rules the probability that the bound holds 
(e.g. low delta 0.01, it holds with probability 0.99)
‚Ä¢ Now we can see how it can ‚Äúexplain‚Äù the underfitting and overfitting and the aspects that control them. 

Intuition:
‚Ä¢ Higher l (data) -> lower VC confidence and bound close to R
‚Ä¢ Too simple model (low VC-dim) can be not suff. due to high Remp (underfitting)
‚Ä¢ Higher VC-dim (fix l) -> lower Remp but VC-conf. and hence R may increase (overfitting)

‚Ä¢ Concept of control of the model complexity (flexibility):
trade-off between model complexity and TR accuracy (fitting)

Discussion Complexity control
‚Ä¢ Statistical Learning Theory (SLT):
	‚Äì Permette inquadramento formale del problema della generalizzazione e underfitting/overfitting, 		fornendono limitazioni superiori analitiche e quantitative al rischio R di predizione 
	su tutti i dati,  indipendemente dal tipo di learning algorithm o dettagli del modello.
	‚Äì Il ML √® ben fondato:
		‚Ä¢ Il rischio del learning (e l‚Äôerrore di generalizzazione) 
		pu√≤ essere analiticamente limitato, e solo pochi concetti sono fondamentali!
		‚Ä¢ Si pu√≤ trovare un buona approssimazione del f target da esempi, 
		pur di avere un buon numero di dati e una adeguata complessit√† del modello 
		(misurabile formalmente con la VC-dim)
	‚Äì Porta a nuovi modelli (SVM) (e altri metodi che direttamente considerino 
	il controllo della complessit√† nella costruzione del modello)
	‚Äì Fonda uno dei principi induttivi sul controllo della complessit√†

Domande aperte:
‚Ä¢ Quali (altri) principi vi sono per fondare il controllo della complessit√† e come operare in pratica?
	‚Äì Come misurare la complessit√† (flessibilit√† per il fitting)?
	‚Äì Come trovare il bilanciamento ottimo tra fitting e complessit√†?

Some Examples for Complexity Control
‚Ä¢ Linear models (LM):
	‚Äì (seems related to) number of free parameters w: input dimension/dim. of the basis expansion 
	(e.g. polynomial degree)
	‚Äì Lambda parameter for the regularized version
‚Ä¢ Decision trees (DT): number of nodes
‚Ä¢ We will see: direct approach to the complexity optimization through the SVM model

Conclusioni
‚Ä¢ ML models flexiblity
	‚Äì Using the power of ML without control is a way to produce illusory results
	‚Äì Control the tradeoff between model fitting and complexity
	‚Äì Fundamental role of validation approaches (for model selection and estimations)
‚Ä¢ Il ML √® ben fondato teoricamente
	‚Äì Domande fondamentali tramite Statistical Learning Theory
	‚Äì ed altre (e.g. PAC-probably approximately correct learning)

Introduction to the use of SVM

Support Vector Machine (SVM) intro
‚Ä¢ A classifier derived from statistical learning theory
‚Ä¢ After years of theoretical developments, SVM became famous when, using images as input, 
it gave accuracy comparable to SotA neural-network (in the 90s) with hand-designed features 
in a handwriting recognition task
‚Ä¢ Currently, SVM is widely used all the application fields of supervised learning
	‚Äì Also used for regression (will not cover)

SVM?
‚Ä¢ you may be tempted to use it (e.g. popular sw tools)
‚Ä¢ After all, it is close to our view of linear models...
‚Ä¢ And an opportunity to see at least 1 model at the state-of-the-art

Indeed we restart from linear model for classification.
We are interested in enrich it for non-linear problems.

Our Aims for SVM intro
Objectives of this SVM intro in IIA are to show:
1) Control of the model complexity via optimization approach
	‚Äì to directly approximate the Structural risk minimization
	-> [Max margin classifier]
2) Use efficiently linear basis expansion via kernel
	‚Äì so we obtain another flexible approach for non-linear supervised learning
	-> [Kernel]
3) Avoid typical misinterpretations in the use of SVM
‚Äì Such are overoptimistic beliefs on overfitting and curse-of-dimensionatilty avoidance
	-> [Practice]

Classification by linear decision boundary
The classification may be viewed as the allocation of the input space in decision regions (e.g. 0/1)
Example: linear separator on 2-dim instance space x = (x1, x2) in R^2, f(x) = 0/1 (or -1/+1)

(x) = {1 if wx + w0 >= 0	[0, 1] outputs
       0 otherwise}
h(x) = sign(wx + w0)		[-1, +1] outputs
	-> Linear threshold unit (LTU)

AIM 1
1) Maximum margin classifier
‚Ä¢ Binary classification problem
‚Ä¢ Let us start assuming a linear separable problem
‚Ä¢ and assuming also no noise data (hard margin SVM)

(we will release later these assumptions)

Margin examples
‚Ä¢ Not all the hyperplanes solving the task are equals...
‚Ä¢ Varying the separating hyperplane the margin varies as well.

The margin is (the double of) the distance between the hyperplane and the closest data points.
Toward max margin classifier...
	-> Intuitively: max distance to the data points, max ‚Äúsafe zone‚Äù

Toward max margin optimization problem
Consider the problem to learn a linear model for binary classification, 
i.e. a function h: R^n -> {-1,1} based on examples (xp, yp)

Training problem: find (w, b) such that all points are classified correctly and the margin is maximized

Canonical representation of the hyperplane [Img. 15-1]: 
	(xp, yp) is classified correctly for all p
	<-> w^Txp + b >= 0 if yp = 1 and w^Txp + b < 0 if yp = -1 for all p
w.l.o.g. (it is possible to rescale w, b so that the closest points to the separating hyperplane
satisfy |wT xp + b| = 1, i.e the support vectors):
	w^Txp + b >= 1 if yp = 1 and w^Txp + b <= -1 if yp = -1 for all p
	<-> (w^Txp + b) yp >= 1 for all p <- constraints: all points are correctly classified

Two Useful Facts
‚Ä¢ Margin = 2/|w| [|w|^2 = (w^Tw)]
	maximize margin <-> minimize |w| <-> minimize (|w|^2)/2
‚Ä¢ VC-dim of the SVM is inverse to the margin -> control of model complexity by the margin
The optimal hyperplane is the hyperplane which maximizes the margin (and solves the training problem)

Hard margin SVM: Quadratic optimization problem
Training problem: find (w,b) such that all points are classified correctly and the margin is maximized
Training problem (primal form): 
	minimize (|w|^2)/2 (i.e. w^Tw) 			[Objective function]
	such that (wxp + b) yp ‚â• 1 for all p = 1...l	[Constraints]

quadratic optimization problem (sw packages)

‚Ä¢ Note: Direct minimization of model complexity (optimization function),
‚Ä¢ holding solution (0 errors) in the constraints
‚Ä¢ Note: Objective function is convex in w

Dual Problem
Dual formulation of training: Maximize_Œ± Œ£_iŒ±_i ‚Äì (Œ£_ijŒ±_iŒ±_jy_iy_jx_i^txj)/2	i, j = 1...l
with Œ±_i >= 0, Œ£_iŒ±_iy_i = 0

‚Ä¢ Find optimal Œ±p p=1..l (Lagrangian multipliers) by quadratic programming
‚Ä¢ Convex -> unique solution
‚Ä¢ Computational cost scales with l (num. of data) instead of n (input dimension)
‚Ä¢ Dual formulation (computing alpha values) allows us to show: 
support vectors and a special form of the solution

Solution: the classifier h(x)
‚Ä¢ With the alfa (computed in the dual form) we can compute (w, b)
	w = Œ£Œ±_py_px_p p = 1...l 	b = y_k - w^Tx_k for any Œ±_k > 0
Def. -> [Img. 15-2]
1) Alfa != 0 <-> support vectors (Œ±_p != 0 -> x_p is a support vector)
	The solution is sparse and formulated only in terms of the SVs
	The hyperplane depends only from support vectors!
2) A special form of the solution: it is not even necessary to compute (w,b) explicitly 
to classify the points

Role of the inner product
‚Ä¢ Data enter in form of dot products of pairs of points

Soft Margin
Hard margin for all the points may be too restrictive
Some errors can be allowed for noise tolerance and to provide larger margin
Solution: allow errors introducing slack-variables.

Slack variables Œæ_i can be added to allow misclassification of difficult or noisy data points

Primal training problem: 
minimize (|w|^2)/2 + C‚àôŒ£_pŒæ_p 
such that (wx_p+ b) yp ‚â• 1 - Œæp and Œæp >= 0 for all p

positive Œæ_p indicates an error or too small margin, C > 0 guides the number of allowed errors
(the indices of the Œæ_p computed by the solver)

C: is a user defined hyperparameter!
Low C -> many TR errors are allowed -> possible underfitting
High C -> no TR errors are allowed -> small margin -> possible overfitting
But we lost the automatic approximation of SRM of hard margin!

AIM 2
Up to now only for linear separable problems... And for non-linear cases?

Kernel
2) Use efficiently linear basis expansion via kernel
	‚Äì so we obtain another flexible approach for non-linear supervised learning

Mapping to a High-Dimensional Space
‚Ä¢ Map the data points in the input space to a high-dimensional (so called in SVM) feature space, 
where they can be linearly separable

linear separators here correspond to non-linear separators in original space

‚Ä¢ Using LBE Œ¶(x) instead of x can be used
‚Ä¢ However, we know that using high dimensional feature spaces (large basis function expansion) 
can be computationally unfeasible and more importantly can easily lead to overfitting 
without controlling the dimension of feature space and the complexity of the classier 
(complexity is here related to the input dimensionality, # of free parameters on the eq:)
	h_w(x) = sign(Œ£_kw_kŒ¶_k(x))
‚Ä¢ We are going to propose the Kernel approach to manage (implicitly) the feature space 
in the context of regularized modeling (where the complexity is margin dependent):
	- Thus, thanks to the automatized regularization of SVM, complexity of the classifier 
	can be kept small regardless of dimensionality in the new feature space.

Use Œ¶(x) instead of x
‚Ä¢ In SVM it is not necessary to compute w and the data enter in form of dot products of pairs of points
and it is not even necessary to compute directly phi
‚Ä¢ i.e. we can implicitly manage the feature space by a Kernel function
	[Img. 15-4]

Kernel
A kernel k: R^n x R^n -> R is a function such that some (possibly high dimensional) Hilbert space X 
and a function Œ¶: R^n ‚Üí X exists with 
	k(xi, xj) = Œ¶(xi)^TŒ¶(xj)
i.e. a kernel is a potential shortcut to compute the dot product efficiently also in high dimensional spaces

We use the K function to compute directly the dot products in the feature space

Well-known popular Kernels
‚Ä¢ Linear: K(xi, xj) = xi^Txj
	‚Äì Mapping Œ¶: x ‚Üí ùúô(x), where ùúô(x) is x itself
‚Ä¢ Polynomial of power p: K(xi, xj)= (1+ xi^Txj)^k
	‚Äì Mapping Œ¶: x ‚Üí ùúô(x), where ùúô(x) has exponential dimension in k
‚Ä¢ RBF (radial-basis function) Gaussian: [Img. 15-5]
	‚Äì Mapping Œ¶: x‚Üí ùúô(x), where ùúô(x) is infinite-dimensional
	- RBF is a very popular choice: note it has a hyperparameter (sigma)
	- Very flexible, it can be used to make decision boundary around each points!
	- Low sigma -> narrow peaked Kernels -> patterns are similar only if very close
	and the classifier replies with the class of the closest point
	- Can be very powerful on TR discrimination but of course prone to overfitting

SVM completed
‚Ä¢ We choose the trade-off parameter C, and the kernel function K (and its parameters)
‚Ä¢ solve the optimization problem to find alfa
	‚Äì Computational cost scales with l (num. of data) instead of n (feature space dimension)
	‚Äì Modularity: change just the Kernel (with the same solver)
‚Ä¢ The final model: 
	H(x) = sign(Œ£_(p in SV) alfa_py_p K(xp, x))

AIM 3
- Practice

Avoid misinterpretation
‚Ä¢ Avoid typical misinterpretation in the use of SVM
	‚Äì Such are overoptimistic beliefs on overfitting and course-of-dim avoidance
‚Ä¢ Overfitting can occur without a careful selection of SVM hyperparameters: C, kernel, kernel parameters...
‚Ä¢ Implicit treatment of High dim space is in the feature space not in the input space 
(assuming we project therein significant inputs)
‚Ä¢ Validation techniques seen so far for model selection (e.g. C, kernel and kernel hyperparameters)
and model evaluation should be used rigorously as well

From LIBSVM guide (grid search suggestion):
- Transform data to the format of an SVM software
- Conduct simple scaling on the data
- Consider the RBF kernel K(x, y) = e^(-gamma||x-y||^2)			[gamma = 1/(2*sigma^2)]
- Use cross-validation to find the best parameter C and gamma
- Use the best parameter C and gamma to train the whole training set
- Test on a separate external set

Details
‚Ä¢ Data preprocessing:
	‚Äì {red, green, blue} ‚Üí (0, 0, 1), (0, 1, 0), (1, 0, 0) 		[1-of-k]
	‚Äì Linear scaling continuous values in range [-1, +1] or [0, 1]
	‚Äì [e.g. v-min/(max-min)]
‚Ä¢ Model selection grid-search: e.g. C and gamma in RBF
	‚Äì With a table on the combinations of all the possible growing (exponential) values 
	to find good intervals, e.g.:
		C = 2^(-5), 2^(-3), ..., 2^15
		gamma = 2^(-15), 2^(-13), ..., 2^3
	- Then a finer grid-search can be performed

Conclusion
‚Ä¢ SVM is very useful and popular advanced ML tool
	‚Äì Performance: often good, but not always the comparison is
favorable wrt other ML methods (NN and others).
‚Ä¢ Coming from theory (SRM) is a good way to provide new ML approaches
‚Ä¢ Combing the use efficiently linear basis expansion via kernel within a max margin approach 
allows to combine flexible models and the control of complexity
‚Ä¢ Modularity of the kernel open new possibilities
‚Ä¢ Avoid to think SVM outside of the validation needs!

K-NN, Unsupervised learning and other approaches

AIM
‚Ä¢ This introductory course cannot cover all the aspects of the large area of Machine Learning
‚Ä¢ We left a series of paradigms, models, and algorithms
‚Ä¢ Let‚Äôs us have a look to some important (not seen so far) cases in the
	‚Äì Instance based approaches
	‚Äì Unsupervised learning tasks
	‚Äì Other...

An overview (for supervised learning)
‚Ä¢ Discrete spaces:
	‚Äì e.g. Conjuction of literals, Decision trees (propositional rules)
	‚Äì Inductive grammars, Inductive Logic Programming...
‚Ä¢ Continuous spaces:
	‚Äì E.g. Multiple Linear Regression, Linear threshold unit (LTU), polynomial regression, k-means...
	‚Äì SVM, Neural Networks
‚Ä¢ Probabilistic/Generative
	‚Äì Traditional parametric models: density estimation (e.g. coin toss), discriminant analysis,
	polynomial regression...
	‚Äì Graphical models: Bayesian networks, Na√Øve Bayes, Markov models, Hidden Markov models...
‚Ä¢ Instance-based
	‚Äì Nearest neighbor

K-NN: K-Nearest Neighbors (supervised learning)

1-Nearest Neighbor
‚Ä¢ Simply store the training data <xp, yp> p= 1...l
‚Ä¢ Given an input x (with dimension n)
‚Ä¢ Find the nearest training example xi
	‚Äì Find i so that we have min d(x, xi) -> i(x) = arg min d(x, xp)
	- E.g. Euclidian distance
‚Ä¢ Then output yi

1-nn
‚Ä¢ Very flexible
‚Ä¢ No misclassifications in TR data -> 0 training error 
	- what for test data?
‚Ä¢ Decision boundaries is not linear: but it is quite irregular
‚Ä¢ May be unnecessary noisy

K-Nearest Neighbors
‚Ä¢ A natural way to classify a new point is to have a look at its neighbors, and take an average:
	avg(x) = 1/k Œ£_(xi in Nk(x)yi
where Nk(x) is a neighborhood of x that contains exactly k neighbors (closest patterns according to d):
	‚Äì k-nearest neighborhood: K-nn.
‚Ä¢ If there is a clear dominance of one of the classes in the neighborhood of an observation x, 
then it is likely that the observation itself would belong to that class, too. 
Thus the classification rule is the majority voting among the members of Nk(x). As before,
	h(x) = {1 if avg(x) > 0,5
		0 otherwise}
‚Ä¢ For regression task: use directly the avg: mean over K-nn 

15-nn
The predicted class is chosen by majority vote amongst the 15-nearest neighbors.
‚Ä¢ Still very flexible
‚Ä¢ Some misclassifications in TR data
‚Ä¢ Decision boundaries is not linear: it is still quite, although less, irregular
‚Ä¢ Decision boundary adapts to the local densities of the classes

Complexity of K-NN
‚Ä¢ Again... there is trade-off between underfitting and overfitting by the possible values of K
‚Ä¢ Classic u-shape curve of the test error moving between a
	‚Äì extremely flexible case (K=1)
	‚Äì up to a very rigid model (K=l): 1 mean for all the data

K-nn: An extreme
‚Ä¢ Not a global hypothesis for all the instances -> no model to be fit
	‚Äì We need to memorize all the input examples
	‚Äì Non parametric model (in contrast think to the parametric linear model 
	with a set of fixed size parameters w)
‚Ä¢ Local estimations (by locally constant functions) vs global linear approximation/estimation 
of the target function (over the instance space)
‚Ä¢ Lazy, memory based, instance-based, distance-based methods

Limits of K-nn: computational cost
‚Ä¢ Note that K-nn makes the local approximation to the target function for each new example to be predicted:
The computational cost is deferred to the prediction phase!

Moreover: high retrieval cost
‚Ä¢ Computationally intensive for each new input: 
computing the distances from the test sample to all stored vectors
	‚Äì The time is proportional to the number of stored patterns
	‚Äì ‚Äúad-hoc‚Äù proximity search algorithms to optimize
		‚Äì> E.g. by indexing the patterns
‚Ä¢ Cost in space (all the data are stored)

Limitations: Curse of dimensionality
‚Ä¢ When we have a lot of input variables (high n), 
K-nn methods often fail because of the curse of dimensionality
	‚Äì when the dimensionality d increases, the volume (side^d) of the space 
	increases so fast that the available data become sparse.
	‚Äì i.e. the amount of data needed to support the result 
	often grows exponentially with the dimensionality.
‚Ä¢ Irrelevant features: The Curse of Noisy
	‚Äì if the target depends on only few of many features in x (e.g. 2 out 20), we could retrieve 
	a ‚Äúsimilar pattern‚Äù with the similarity dominated by the large number of irrelevant features
	- It grows with the dimensionality

Distance based methods: a consideration and inductive bias
Distance or metric based approaches:
‚Ä¢ Common line among e.g. K-means, K-NN and kernel-based approaches
‚Ä¢ What is similar? Measure when couple of patterns are similar.
‚Ä¢ Giving a metric (e.g. the Euclidian metric was assumed so far) 
poses a relevant bias on the solution (much more than the alg. details)
‚Ä¢ The metric is typically domain dependent
	‚Äì E.g. two string in a language, in biology, etc.
	‚Äì Based on the prior knowledge of the domain expert
	‚Äì In some way, the same of feature engineering to represent the problem
‚Ä¢ Looking forward: is it possible to learn the metric? Yes, NN, Learning Kernels...

K-means
Unsupervised learning

Tasks: Unsupervised Learning
‚Ä¢ Unsupervised Learning: No teacher!
	- TR = set of unlabeled data <x>
‚Ä¢ E.g. to find natural groupings in a set of data
	‚Äì Clustering (partition of data into clusters [subsets of similar data])
	‚Äì Dimensionality reduction/Visualization/Preprocessing
	‚Äì Modeling the data density	

Utility
In nature:
	‚Äì Example (Taxonomy): organize life forms: what are mammals?
	‚Äì Example (human learning): babies learn to recognize (‚Äúcluster‚Äù) familiar faces
	before they can understand human speech

In Data Analysis:
‚Ä¢ Explorative data analysis
	‚Äì Discover data latent structure automatically
	‚Äì Find unknown patterns
‚Ä¢ Preprocessing for other ML approaches
	‚Äì E.g. by dimensionality reduction
‚Ä¢ Moreover: Unlabeled data is much cheaper to obtain
	‚Äì Examples: e.g. 10 million images (sampled frames from YouTube) 
	with state-of-the-art results on object recognition (22,000 categories)

Clustering
Partition of data into clusters/prototypes (subsets of ‚Äúsimilar‚Äù data):
patterns within a valid cluster are more similar to each other 
than they are to a pattern belonging to a different cluster

H space for clustering 
‚Ä¢ Goal: optimal partitioning of unknown distribution in x-space into regions (clusters) 
approximated by a cluster center or prototype.
‚Ä¢ H: a set of vector quantizers x -> c(x) (that represent the cluster)
	- continuous space -> discrete space
‚Ä¢ Example of Loss function: measures the vector quantizer optimality
‚Ä¢ A common loss function would be the squared error distortion:
	L(h(xp)) = ||xp - c(xp)||^2

‚Ä¢ The average value over the distribution of inputs 
is the average distortion or reconstruction or quantization error

K-means
‚Ä¢ The k-means is the simplest and most commonly used algorithm employing a squared error criterion
‚Ä¢ The k-means algorithm is popular because it is easy to implement, and it's in general efficient
‚Ä¢ However, it has several drawbacks and limitations that we will see

Basic K-means Batch Alg.
LBG or generalized Lloyd algorithm
(1) Choose k cluster centers to coincide with k randomly chosen patterns 
or k randomly defined points inside the hypervolume containing the pattern set.
(2) Assign each pattern to the closest cluster center (the winner).
(3) Recompute the cluster centers (geometric centroid, i.e. mean) using the current cluster memberships.
(4) If a convergence criterion is not met, go to step 2. E.g.
	‚Äì no (or minimal) reassignment of patterns to new cluster centers,
	‚Äì minimal decrease in squared error.

Details
‚Ä¢ Given the cluster centers c1...cK (vectors of dim n, as x)
‚Ä¢ (2) For each x the winner is (the nearest prototype):
	i*(x) = arg min ||x - ci||^2
‚Ä¢ Now x belong to cluster i*
‚Ä¢ (3) For each cluster i the new mean (centroid) is:
	ci = \/|clusteri| * Œ£_(j: xj in clusteri) xj

K-means limitations
‚Ä¢ The number of clusters to find must be provided
	‚Äì Trials and-error to find the most suitable K
‚Ä¢ Local minima of loss L(x) makes the method dependent on initialization
	‚Äì Run multiple times from different random initializations
	‚Äì Initialize with a heuristic
‚Ä¢ It can work well for compact and hyperspherical clusters.
‚Ä¢ No visualization properties
	‚Äì K-means does not allow us to project data in a lower dim space

Evaluation
‚Ä¢ How is the output of a clustering algorithm evaluated?
‚Ä¢ What characterizes a ‚Äògood‚Äô clustering result and a ‚Äòpoor‚Äô one?
‚Ä¢ Subjective: little in the way of ‚Äògold standards‚Äô
exists in clustering except in well-prescribed subdomains.
‚Ä¢ Objective measures (no treated here): e.g. quantization error 
(but note that depends on the number of centroids)!
‚Ä¢ But not ‚Äútrivially‚Äù the label of a underlining classification task, 
called purity (entropy, mutual inf...) (else you can use a classifier)!
‚Ä¢ Quality of model/clustering versus quality of results (domain dependent)

Other topics in Unsupervised
‚Ä¢ Hierarchical Clustering -> dendrograms
‚Ä¢ Principal Components, Curves and Surfaces
‚Ä¢ Association Rules
‚Ä¢ Independent Component Analysis and Exploratory Projection Pursuit
‚Ä¢ Multidimensional Scaling
‚Ä¢ ART (adaptive resonance theory) NN
‚Ä¢ Self-organizing map (SOM) neural networks

Other approaches for preprocessing
Among the multitude of approaches it deserve to mention in this course:

‚Ä¢ Dimensionality reduction (in unsup. Learning)
	‚Äì <x1,x2, ..., xn> -> <x'1, x'2, ..., x'n'> 	n > n‚Äô
	‚Äì where the new features can be a combination of the original ones.
	- E.g. PCA Principal Component Analysis
		‚Äì> New axis are computed along the directions of max variance 
		of the original manifold of data
‚Ä¢ Feature selection
	‚Äì Where we choose a subset of all the features
	‚Äì Domain knowledge (features engineering)
	‚Äì Automatic features selection on the basis of their redundancy 
	or relevance in the task (the most informative)
	‚Äì Many approaches: The problem is difficult as the learning problem one.
‚Ä¢ Outlier detection
	‚Äì Find unusual data values that are not consistent with most observations 
	(e.g. due to abnormal measurements errors)

Other tasks
‚Ä¢ Reinforcement Learning (learning with right/wrong critic).
	‚Äì Adaptation in autonomous systems (especially in robotics)
	‚Äì ‚Äúthe algorithm learns a policy of how to act given an observation of the world. 
	Every action has some impact in the environment, and the environment provides feedback 
	that guides the learning algorithm‚Äù.
	‚Äì Instead of supervision for each step we have information about wins/losses 
	(rewards or punishment) for the final state
	‚Äì Actions need to maximize the amount of received rewards
	‚Äì The learning decide which actions were most responsible for wins/losses
‚Ä¢ Semi-supervised learning
	‚Äì combines both labeled and unlabeled examples (typically in greater number) 
	to generate an appropriate function or classifier.
‚Ä¢ Learn to rank: (e.g. for search engines)
	‚Äì When the input is a set of objects and the desired output is a ranking of those objects
‚Ä¢ On-line learning: new examples are learned in time
‚Ä¢ Structured domain learning and relational learning:
	‚Äì The input and/or output domain can be structured 
	in the form of sequences (signals, time series...)
	‚Äì Or even more complex: trees, graphs, networks.
		-> Vector patterns + relationships with other components
		-> Structured Input: Molecules, web pages as graph/network data...
		-> Structured Output: network data in social analysis, Parse tree from sentences...
‚Ä¢ Neural Networks: for both supervised and unsupervised learning
	- Close to our view of Linear Threshold Unit (LTU):
	- Indeed, LTU is close to the basic unit of a Artificial NN, the perceptron.
	- A NN is a network of such non-linear units
		‚Äì> with universal approximation capability
	- Composing a very Flexible approach for any purpose of the ML.
		‚Äì> Gradient descent approach for learning (backprogation)

Neural Networks
Natural inspired models: Neural Networks
	‚Äì Studied as computing paradigms since the '40
	‚Äì Neurobiological inspiration
	‚Äì Nowadays: set of powerful computing models for function approximation 
	and with predictive capabilities supported by a rigorous theoretical ground (learning theory)

Why internal layers?
The internal (hidden) layer with non-linear units provides the NN with:
‚Ä¢ The capability to extract by learning a new representation of data (learning abstract features)
‚Ä¢ The new representations simplify the classification task at the last layer
‚Ä¢ Non-linear in w adaptive basis expansion where the phi are learned (depend on w)! 
(but unfortunately it also leads to a non-linear optimization problem)
‚Ä¢ Distributed representation (versus symbolic): similarity can be more easily 
treated by real values vectors
‚Ä¢ Very flexible approach for non-linear tasks
‚Ä¢ For both classification and regression tasks
‚Ä¢ Universal approximation capability

Deep Learning (DL)
‚Ä¢ E.g. by deep multilayer NN architectures
‚Ä¢ Common aspects among different approaches:
	‚Äì Multiple layers of nonlinear processing units
	‚Äì Supervised or unsupervised learning of feature representations in each layer, 
	with the layers forming a hierarchy from low-level to high-level features/representations
	(different levels of abstraction).

Learning representations of data
‚Ä¢ Representation learning versus the feature engineering approach (see Distance based methods)
‚Ä¢ Automatically discover the representations (a.k.a. the features) needed for detection or classification
‚Ä¢ Increase level of abstraction through different layers

Example: An observation (e.g., an image) can be represented in many ways such as a vector of intensity
values per pixel, or in a more abstract way as a set of edges, regions of particular shape, etc.
	- Edges form motifs, motifs assemble into parts (e.g. corner), 
	parts form specific objects (e.g. eyes)...

‚Ä¢ Performs best when input information has some form of structure, 
e.g. spatial, temporal... (images, language, music...)

Combine to generalize
‚Ä¢ And the abstract features can be ‚Äúreused‚Äù at the higher levels.
‚Ä¢ It can combine them to generalize to unseen (during training) cases:
	‚Äì Woman without glasses
	‚Äì Man with glass
		-> Woman with glass can be easily represented as well

DL advantages
‚Ä¢ Exploit compositionality of internal representations:
	‚Äì An exponential gain in representational power, due to the fact that simpler concepts 
	represented in a layer of the network can be exploited as primitives many times by the next layer
	to represent more complex concepts, avoiding explicit combinatorial representation 
	(and learning) of features
	‚Äì E.g. you have to learn each word/sub-pattern but not directly all the possible combinations 
	(i.e. without having to see all the configurations), hence: 
	less examples are needed to reach good generalization capability!

DL results
‚Ä¢ Deep network were difficult to be trained in the past, but combining:
	‚Äì improvement in the techniques for training of such huge models,
	‚Äì HPC (e.g. GPU) and
	‚Äì large data collections from industry and real applications (e.g. millions of images)
‚Ä¢ they nowadays work very well making a revolution in the AI approach to real-world solutions
‚Ä¢ ‚ÄúThese methods have dramatically improved the state-of-the-art in speech recognition, 
visual object recognition, object detection and many other domains‚Äù

Find-S: 		language bias + search bias
Candidate-Elimination: 	language bias

Linear Models: 		language bias + search bias

ID3 (Decision Trees): 	search bias

SVM:

K-NN:			metric bias
K-Means:
```
