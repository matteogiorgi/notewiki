# Esercizio 6


## Modelli visti

- regole congiuntive
- modelli lineari
- lbe
- ridge regression
- alberi di decisione




## Criteri da valutare

1. criteri informatici classici
     a. complessità in tempo
     b. complessità in spazio

2. criteri specifici del ML
     I. flessibilità modello -> il modello è accurato?
    II. generalizzazione     -> il modello è capace di generalizzare sui nuovi dati?




### regole congiuntive (1) vs modelli lineari (2)

 a. (2) wins     => mi costa di più una ricerca esaustiva nello spazio delle ipotesi congiuntive (candidate elimination)
                    di un ciclo di k iterazioni dove sommo k volte l'errore quadro
 b. (2) wins     => candidate elimination si deve tenere in memoria tutte le ipotesi utili del version space
                    mentre il modello lineare semplicemente calcola l'equazione di una retta
 I. (1) (2) draw => entranbi hanno un forte bias di linguaggio
II. (2) wins     => (1) non può proprio rappresentare alcuni target concept mentre (2) può sempre approssimare
                    al meglio un insieme di punti ad una retta


### regole congiuntive (1) vs lbe (2)

 a. (2) wins     => mi costa di più una ricerca esaustiva nello spazio delle ipotesi congiuntive (candidate elimination)
                    di un ciclo di k iterazioni dove sommo k volte la *phi*
 b. (2) wins     => candidate elimination si deve tenere in memoria tutte le ipotesi utili del version space
                    mentre l'LBE calcola l'equazione di una funzione
 I. (2) wins     => l'LBE è un'evoluzione del modello lineare per migliorarne la flessibilità
II. (1) wins     => a causa dell'eccessivo *fit* dell'LBE, si rischia di perdere la capacità di generalizzazione del
                    modello (overfitting)


### regole congiuntive (1) vs ridge regression (2)

 a. (2) wins     => (2) è un miglioramento di LBE
 b. (2) wins     => `come sopra`
 I. (2) wins     => `come sopra`
II. (2) wins     => grazie alla ridge regression si limita la complessità del modello,
                    ottennendo una buona capacità di generalizzazione


### regole congiuntive (1) vs alberi di decisione (2)

 a. (1) wins     => ID3 deve calcolare l'Entropia, l'Information gain
                    e ad ogni passo deve esplorare un intero albero
 b. (1) wins     => le ipotesi congiuntive richidono un minore spazio di memoria per essere rappresentate
 I. (2) wins     => (2) non ha bias di linguaggio quindi ha più flessibilità perchè faccio una selezione
                    delle ipotesi in fase di ricerca
II. (2) wins     => (2) ha la possibilità di includere ipotesi più generiche


### modelli lineari (1) vs lbe (2)

 a. (1) (2) draw => Entrambi i modelli sono lineari nei parametri e usano lo stesso algoritmo di learning
                    (il modello lineare è semplicemente un LBE con *phi(x)=x*)
 b. (1) (2) draw => `come sopra`
 I. (2) wins     => (2) è più flessibile perchè in *phi* ci posso mettere la funzione che voglio
II. (1) wins     => (2) potrebbe causare overfitting (a causa del mancato controllo della complessità)
                    mentre (1) usa solamente rette (*phi(x)=x*) quindi potrebbe andare in underfitting
                    ma mai in overfitting


### modelli lineari (1) vs ridge regression (2)

 a. (1) (2) draw => Asintoticamente (1) e (2) si comportano allo stesso modo
 b. (1) (2) draw => `come sopra`
 I. (2) wins     => (2) è più flessibile perchè in *phi* ci posso mettere la funzione che voglio
II. (2) wins     => (2) potrebbe causare overfitting ma grazie alla regolarizzazione (*lambda*),
                    si limita la complessità per qualsiasi tipo di funzioni


### modelli lineari (1) vs alberi di decisione (2)

 a. (1) wins     => (2) deve calcolare l'Entropia, l'Information gain e ad ogni passo deve esplorare
                    un intero albero quindi (1) è più semplice
 b. (1) wins     => (1) richiedono minore spazio di memoria per essere rappresentati
 I. (2) wins     => (2) non ha bias di linguaggio quindi ha più flessibilità perchè faccio
                     selezione delle ipotesi in fase di ricerca
II. (2) wins     => (2) ha la possibilità di includere ipotesi più generiche


### lbe (1) vs ridge regression (2)

 a. (1) (2) draw => Asintoticamente (1) e (2) si comportano allo stesso modo
 b. (1) (2) draw => `come sopra`
 I. (1) (2) draw => (1) e (2) offrono le stesse possibilità in termini di flessibilità
II. (2) wins     => (2) potrebbe causare overfitting ma grazie alla regolarizzazione (*lambda*),
                        si limita la complessità per qualsiasi tipo di funzioni


### lbe (1) vs alberi di decisione (2)

 a. `(1)` wins     => (2) deve calcolare l'Entropia, l'Information gain e ad ogni passo deve esplorare
                    un intero albero quindi (1) è più semplice
 b. (1) wins     => (1) richiede minore spazio di memoria per essere rappresento
 I. (1) (2) draw => (1) e (2) offrono le stesse possibilità in termini di flessibilità
II. (2) wins     => (1) potrebbe andare in overfitting perchè non include il controllo della complessità


### ridge regression (1) vs alberi di decisione (2)

 a. (1) wins     => (2) deve calcolare l'Entropia, l'Information gain e ad ogni passo deve esplorare
                    un intero albero quindi (1) è più semplice
 b. (1) wins     => (1) richiede minore spazio di memoria per essere rappresento
 I. (1) (2) draw => (1) e (2) offrono le stesse possibilità in termini di flessibilità
II. (1) (2) draw => (1) e (2) controllano l'overfitting con iperparametri (*lambda* per (1) e *#nodi* per (2))
